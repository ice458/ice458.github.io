#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ã‚µã‚¤ãƒˆãƒãƒƒãƒ—è‡ªå‹•ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ - ice458ã®ç‰©ç½®ã
ã‚µã‚¤ãƒˆå†…ã®HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•æ¤œå‡ºã—ã¦XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
"""

import os
import re
from datetime import datetime
from pathlib import Path
import xml.etree.ElementTree as ET
from xml.dom import minidom

class SitemapGenerator:
    def __init__(self, base_url="https://ice458.github.io", root_dir=None):
        self.base_url = base_url.rstrip('/')
        self.root_dir = Path(root_dir) if root_dir else Path(__file__).parent
        self.sitemap_data = []

        # å„ªå…ˆåº¦ã¨changefreqã®è¨­å®š
        self.page_configs = {
            'index.html': {'priority': 1.0, 'changefreq': 'weekly'},
            'blog.html': {'priority': 0.8, 'changefreq': 'weekly'},
            'links.html': {'priority': 0.6, 'changefreq': 'monthly'},
            'project-': {'priority': 0.7, 'changefreq': 'monthly'},  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒšãƒ¼ã‚¸
            'blog/article-': {'priority': 0.6, 'changefreq': 'yearly'},  # ãƒ–ãƒ­ã‚°è¨˜äº‹
        }

    def get_file_modification_date(self, file_path):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€çµ‚æ›´æ–°æ—¥ã‚’å–å¾—"""
        try:
            timestamp = os.path.getmtime(file_path)
            return datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d')
        except:
            return datetime.now().strftime('%Y-%m-%d')

    def extract_title_from_html(self, file_path):
        """HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                title_match = re.search(r'<title[^>]*>([^<]+)</title>', content, re.IGNORECASE)
                if title_match:
                    return title_match.group(1).strip()
        except:
            pass
        return None

    def get_page_config(self, relative_path):
        """ãƒšãƒ¼ã‚¸ã®è¨­å®šï¼ˆå„ªå…ˆåº¦ã€changefreqï¼‰ã‚’å–å¾—"""
        # å®Œå…¨ä¸€è‡´ã®è¨­å®š
        if relative_path == 'index.html':
            return {'priority': 1.0, 'changefreq': 'weekly'}
        elif relative_path == 'blog.html':
            return {'priority': 0.8, 'changefreq': 'weekly'}
        elif relative_path == 'links.html':
            return {'priority': 0.6, 'changefreq': 'monthly'}

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒã®è¨­å®š
        if relative_path.startswith('project-') and '/index.html' in relative_path:
            return {'priority': 0.7, 'changefreq': 'monthly'}
        elif relative_path.startswith('blog/article-'):
            return {'priority': 0.6, 'changefreq': 'yearly'}

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        return {'priority': 0.5, 'changefreq': 'monthly'}

    def scan_html_files(self):
        """HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        self.sitemap_data = []

        # é™¤å¤–ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
        exclude_patterns = [
            r'__pycache__',
            r'\.git',
            r'\.bak$',
            r'test_.*\.html',
            r'.*_files/',  # bp/å†…ã®ãƒªã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«
            r'bp/.*\.htm$',  # bp/å†…ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—HTMLãƒ•ã‚¡ã‚¤ãƒ«
        ]

        def should_exclude(path_str):
            for pattern in exclude_patterns:
                if re.search(pattern, path_str):
                    return True
            return False

        # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«æ¤œç´¢
        for html_file in self.root_dir.rglob('*.html'):
            relative_path = html_file.relative_to(self.root_dir)
            relative_path_str = str(relative_path).replace('\\', '/')

            # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
            if should_exclude(relative_path_str):
                continue

            # URLã‚’æ§‹ç¯‰
            if relative_path_str == 'index.html':
                url = f"{self.base_url}/"
            elif relative_path_str.endswith('/index.html'):
                # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚„ãƒ–ãƒ­ã‚°è¨˜äº‹ã®index.htmlã®å ´åˆã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå½¢å¼ã«
                dir_path = relative_path_str[:-11]  # '/index.html'ã‚’é™¤å»
                url = f"{self.base_url}/{dir_path}/"
            else:
                url = f"{self.base_url}/{relative_path_str}"

            # ãƒšãƒ¼ã‚¸è¨­å®šã‚’å–å¾—
            config = self.get_page_config(relative_path_str)

            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
            title = self.extract_title_from_html(html_file)

            # æœ€çµ‚æ›´æ–°æ—¥ã‚’å–å¾—
            lastmod = self.get_file_modification_date(html_file)

            self.sitemap_data.append({
                'url': url,
                'lastmod': lastmod,
                'changefreq': config['changefreq'],
                'priority': config['priority'],
                'title': title,
                'file_path': relative_path_str
            })

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å€‹åˆ¥ã«ãƒã‚§ãƒƒã‚¯ï¼ˆindex.htmlãŒã‚ã‚‹å ´åˆï¼‰
        for project_dir in self.root_dir.glob('project-*'):
            if project_dir.is_dir():
                index_file = project_dir / 'index.html'
                if index_file.exists():
                    relative_path = index_file.relative_to(self.root_dir)
                    relative_path_str = str(relative_path).replace('\\', '/')

                    # æ—¢ã«è¿½åŠ æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
                    if not any(item['file_path'] == relative_path_str for item in self.sitemap_data):
                        url = f"{self.base_url}/{project_dir.name}/"
                        config = self.get_page_config(relative_path_str)
                        title = self.extract_title_from_html(index_file)
                        lastmod = self.get_file_modification_date(index_file)

                        self.sitemap_data.append({
                            'url': url,
                            'lastmod': lastmod,
                            'changefreq': config['changefreq'],
                            'priority': config['priority'],
                            'title': title,
                            'file_path': relative_path_str
                        })

        # å„ªå…ˆåº¦é †ã§ã‚½ãƒ¼ãƒˆï¼ˆé«˜ã„å„ªå…ˆåº¦ã‹ã‚‰ä½ã„å„ªå…ˆåº¦ã¸ï¼‰
        self.sitemap_data.sort(key=lambda x: (-x['priority'], x['url']))

    def generate_xml_sitemap(self):
        """XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆ"""
        # XMLåå‰ç©ºé–“
        ns = "http://www.sitemaps.org/schemas/sitemap/0.9"

        # ãƒ«ãƒ¼ãƒˆè¦ç´ ã‚’ä½œæˆ
        urlset = ET.Element("urlset")
        urlset.set("xmlns", ns)

        # å„URLã‚’è¿½åŠ 
        for item in self.sitemap_data:
            url_elem = ET.SubElement(urlset, "url")

            # locè¦ç´ ï¼ˆå¿…é ˆï¼‰
            loc = ET.SubElement(url_elem, "loc")
            loc.text = item['url']

            # lastmodè¦ç´ 
            lastmod = ET.SubElement(url_elem, "lastmod")
            lastmod.text = item['lastmod']

            # changefreqè¦ç´ 
            changefreq = ET.SubElement(url_elem, "changefreq")
            changefreq.text = item['changefreq']

            # priorityè¦ç´ 
            priority = ET.SubElement(url_elem, "priority")
            priority.text = str(item['priority'])

        return urlset

    def save_sitemap(self, output_file="sitemap.xml"):
        """ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        xml_tree = self.generate_xml_sitemap()

        # XMLã‚’æ•´å½¢
        rough_string = ET.tostring(xml_tree, encoding='unicode')
        reparsed = minidom.parseString(rough_string)
        pretty_xml = reparsed.toprettyxml(indent="  ")

        # ç©ºè¡Œã‚’å‰Šé™¤
        lines = [line for line in pretty_xml.split('\n') if line.strip()]
        formatted_xml = '\n'.join(lines)

        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        output_path = self.root_dir / output_file
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(formatted_xml)

        return output_path

    def print_summary(self):
        """ã‚¹ã‚­ãƒ£ãƒ³çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        print("=== ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ç”Ÿæˆçµæœ ===")
        print(f"æ¤œå‡ºã•ã‚ŒãŸãƒšãƒ¼ã‚¸æ•°: {len(self.sitemap_data)}")
        print(f"ãƒ™ãƒ¼ã‚¹URL: {self.base_url}")
        print(f"ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.root_dir}")
        print()

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®çµ±è¨ˆ
        categories = {}
        for item in self.sitemap_data:
            path = item['file_path']
            if path == 'index.html':
                category = 'ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸'
            elif path == 'blog.html':
                category = 'ãƒ–ãƒ­ã‚°ãƒˆãƒƒãƒ—'
            elif path == 'links.html':
                category = 'ãƒªãƒ³ã‚¯é›†'
            elif path.startswith('project-'):
                category = 'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒšãƒ¼ã‚¸'
            elif path.startswith('blog/article-'):
                category = 'ãƒ–ãƒ­ã‚°è¨˜äº‹'
            else:
                category = 'ãã®ä»–'

            categories[category] = categories.get(category, 0) + 1

        print("ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ:")
        for category, count in categories.items():
            print(f"  {category}: {count}ãƒšãƒ¼ã‚¸")
        print()

        print("å„ªå…ˆåº¦ã®é«˜ã„ãƒšãƒ¼ã‚¸ï¼ˆä¸Šä½10ä»¶ï¼‰:")
        for i, item in enumerate(self.sitemap_data[:10]):
            title = item['title'] or item['file_path']
            if len(title) > 50:
                title = title[:47] + "..."
            print(f"  {i+1:2d}. {title} (å„ªå…ˆåº¦: {item['priority']}, æ›´æ–°: {item['lastmod']})")

        if len(self.sitemap_data) > 10:
            print(f"  ... ä»– {len(self.sitemap_data) - 10} ãƒšãƒ¼ã‚¸")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ice458ã®ç‰©ç½®ã - ã‚µã‚¤ãƒˆãƒãƒƒãƒ—è‡ªå‹•ç”Ÿæˆãƒ„ãƒ¼ãƒ«")
    print("=" * 50)

    # ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’åˆæœŸåŒ–
    generator = SitemapGenerator()

    # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³
    print("HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³ä¸­...")
    generator.scan_html_files()

    # ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    generator.print_summary()

    # ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆãƒ»ä¿å­˜
    print("XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆä¸­...")
    output_path = generator.save_sitemap()

    print(f"âœ… ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ãŒæ­£å¸¸ã«ç”Ÿæˆã•ã‚Œã¾ã—ãŸ: {output_path}")
    print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {output_path.stat().st_size} bytes")

    # robots.txtã®ç¢ºèª
    robots_file = generator.root_dir / "robots.txt"
    if robots_file.exists():
        print("âœ… robots.txt ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
    else:
        print("âš ï¸  robots.txt ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("   æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã«ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’çŸ¥ã‚‰ã›ã‚‹ãŸã‚ã€robots.txt ã®ä½œæˆã‚’æ¨å¥¨ã—ã¾ã™")

    print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. Google Search Console ã«ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’é€ä¿¡")
    print("2. Bing Webmaster Tools ã«ã‚‚é€ä¿¡ï¼ˆæ¨å¥¨ï¼‰")
    print("3. æ–°ã—ã„ãƒšãƒ¼ã‚¸ã‚’è¿½åŠ ã—ãŸã‚‰ã€ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å†å®Ÿè¡Œ")

if __name__ == "__main__":
    main()
